{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch ASR Phoneme Extraction\n",
    "\n",
    "This notebook demonstrates how to extract phoneme representations from speech using PyTorch and pre-trained ASR models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required libraries if they're not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install the packages\n",
    "# !pip install torch torchaudio transformers matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Sample Audio\n",
    "\n",
    "Let's download a sample audio file to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample audio file from LibriSpeech\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "sample_dir = \"sample_data\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "audio_url = \"https://www.openslr.org/resources/12/dev-clean/84/121123/84-121123-0001.flac\"\n",
    "audio_path = os.path.join(sample_dir, \"sample_audio.flac\")\n",
    "\n",
    "if not os.path.exists(audio_path):\n",
    "    print(f\"Downloading sample audio to {audio_path}...\")\n",
    "    urlretrieve(audio_url, audio_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Sample audio already exists at {audio_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Processing Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(file_path):\n",
    "    # Load audio\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    return waveform.squeeze(), sample_rate\n",
    "\n",
    "# Load and process the audio\n",
    "waveform, sample_rate = process_audio(audio_path)\n",
    "\n",
    "# Display audio information\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Waveform shape: {waveform.shape}\")\n",
    "print(f\"Audio duration: {waveform.shape[0]/sample_rate:.2f} seconds\")\n",
    "\n",
    "# Play the audio\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained ASR Model\n",
    "\n",
    "We'll use the Wav2Vec 2.0 model from Facebook, which has been pre-trained on 960 hours of LibriSpeech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and processor\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Phoneme Probabilities\n",
    "\n",
    "Now we'll extract the logits from the model, which represent the probabilities of different phonemes at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phoneme_probs(waveform, sample_rate=16000):\n",
    "    # Process audio for model input\n",
    "    input_values = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "    input_values = input_values.to(device)\n",
    "    \n",
    "    # Get model outputs (without gradient calculation)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    return probs.cpu().squeeze(), processor.tokenizer.decoder\n",
    "\n",
    "# Get phoneme probabilities\n",
    "phoneme_probs, decoder = extract_phoneme_probs(waveform)\n",
    "print(f\"Shape of phoneme probabilities: {phoneme_probs.shape}\")\n",
    "print(f\"Number of time steps: {phoneme_probs.shape[0]}\")\n",
    "print(f\"Number of phoneme classes: {phoneme_probs.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Phoneme Activations\n",
    "\n",
    "Let's visualize the top phoneme activations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phoneme_activations(probs, decoder, top_k=5):\n",
    "    # Get top-k phonemes at each time step\n",
    "    top_probs, top_indices = torch.topk(probs, k=top_k, dim=1)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    top_probs = top_probs.numpy()\n",
    "    top_indices = top_indices.numpy()\n",
    "    \n    # Get phoneme labels\n",
    "    phoneme_map = {v: k for k, v in decoder.items()}\n",
    "    \n",
    "    # Create a time axis (assuming 50 frames per second for Wav2Vec 2.0)\n",
    "    time_steps = np.arange(top_probs.shape[0]) / 50\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot for a subset of time steps for clarity\n",
    "    start_idx = 0\n",
    "    end_idx = min(200, len(time_steps))  # Show first 4 seconds or less\n",
    "    \n",
    "    for i in range(top_k):\n",
    "        plt.plot(time_steps[start_idx:end_idx], \n",
    "                 top_probs[start_idx:end_idx, i], \n",
    "                 label=f\"Class {top_indices[0, i]} ({phoneme_map.get(top_indices[0, i], '')})\")\n",
    "    \n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Top Phoneme Activations Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize phoneme activations\n",
    "plot_phoneme_activations(phoneme_probs, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding to Phonemes and Text\n",
    "\n",
    "Now let's decode the model outputs to both phonemes and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_outputs(probs, decoder):\n",
    "    # Get the most likely phoneme at each time step\n",
    "    pred_ids = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    # Decode to phonemes (keeping all predictions)\n",
    "    phoneme_sequence = [decoder.get(id.item(), f\"[{id.item()}]\") for id in pred_ids]\n",
    "    \n",
    "    # Apply CTC decoding logic (collapse repeated tokens and remove blanks)\n",
    "    collapsed_phonemes = []\n",
    "    prev_id = -1\n",
    "    for id in pred_ids:\n",
    "        if id != prev_id and id != 0:  # 0 is usually the blank token in CTC\n",
    "            collapsed_phonemes.append(decoder.get(id.item(), f\"[{id.item()}]\"))\n",
    "        prev_id = id\n",
    "    \n",
    "    # Join phonemes to get the text\n",
    "    text = ''.join(collapsed_phonemes).replace('|', ' ')\n",
    "    \n",
    "    return phoneme_sequence, collapsed_phonemes, text\n",
    "\n",
    "# Decode outputs\n",
    "phoneme_sequence, collapsed_phonemes, text = decode_outputs(phoneme_probs, decoder)\n",
    "\n",
    "print(\"Full phoneme sequence (first 50 frames):\")\n",
    "print(phoneme_sequence[:50])\n",
    "print(\"\\nCollapsed phoneme sequence:\")\n",
    "print(collapsed_phonemes)\n",
    "print(\"\\nDecoded text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Model Fine-tuned for Phoneme Recognition\n",
    "\n",
    "For a more direct approach to phoneme recognition, we can use a model specifically fine-tuned for phoneme recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model fine-tuned for phoneme recognition\n",
    "# Note: This will download a different model\n",
    "phoneme_model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "print(f\"Loading phoneme model: {phoneme_model_name}\")\n",
    "\n",
    "try:\n",
    "    phoneme_processor = Wav2Vec2Processor.from_pretrained(phoneme_model_name)\n",
    "    phoneme_model = Wav2Vec2ForCTC.from_pretrained(phoneme_model_name).to(device)\n",
    "    print(\"Phoneme model loaded successfully!\")\n",
    "    \n",
    "    def transcribe_to_phonemes(waveform, sample_rate=16000):\n",
    "        # Process audio for model input\n",
    "        input_values = phoneme_processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "        input_values = input_values.to(device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = phoneme_model(input_values).logits\n",
    "        \n",
    "        # Decode phonemes\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        phoneme_string = phoneme_processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        return phoneme_string\n",
    "\n",
    "    # Get phoneme transcription\n",
    "    phoneme_transcription = transcribe_to_phonemes(waveform)\n",
    "    print(\"\\nPhoneme transcription:\")\n",
    "    print(phoneme_transcription)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading phoneme model: {e}\")\n",
    "    print(\"Skipping phoneme-specific model demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Phoneme Distributions\n",
    "\n",
    "Let's analyze the distribution of phonemes in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count phoneme occurrences\n",
    "from collections import Counter\n",
    "\n",
    "# Count non-blank phonemes\n",
    "phoneme_counts = Counter([p for p in collapsed_phonemes if p != ''])\n",
    "\n",
    "# Plot top 15 phonemes\n",
    "top_phonemes = phoneme_counts.most_common(15)\n",
    "phonemes, counts = zip(*top_phonemes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(phonemes, counts)\n",
    "plt.title('Top 15 Phonemes in Sample')\n",
    "plt.xlabel('Phoneme')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Load and process audio files for ASR\n",
    "2. Extract phoneme probabilities from Wav2Vec 2.0 models\n",
    "3. Visualize phoneme activations over time\n",
    "4. Decode phoneme sequences to text\n",
    "5. Use models specifically fine-tuned for phoneme recognition\n",
    "\n",
    "These techniques can be applied to various applications such as:\n",
    "- Studying pronunciation patterns\n",
    "- Developing language learning tools\n",
    "- Creating more interpretable ASR systems\n",
    "- Analyzing speech disorders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}