{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch ASR Phoneme Extraction\n",
    "\n",
    "This notebook demonstrates how to extract phoneme representations from speech using PyTorch and pre-trained ASR models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required libraries if they're not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install the packages\n",
    "!pip install torch torchaudio transformers matplotlib numpy soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Try to install required packages for audio processing\n",
    "try:\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "except ImportError:\n",
    "    print(\"Installing librosa and soundfile for audio processing...\")\n",
    "    !pip install librosa soundfile\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Sample Audio\n",
    "\n",
    "Let's download a sample audio file to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract a sample audio file from LibriSpeech\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from urllib.request import urlretrieve\n",
    "import shutil\n",
    "\n",
    "sample_dir = \"sample_data\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# Target audio file paths - we'll create both FLAC and WAV versions\n",
    "flac_path = os.path.join(sample_dir, \"sample_audio.flac\")\n",
    "wav_path = os.path.join(sample_dir, \"sample_audio.wav\")\n",
    "\n",
    "# Check which files exist and set the audio path accordingly\n",
    "flac_exists = os.path.exists(flac_path)\n",
    "wav_exists = os.path.exists(wav_path)\n",
    "\n",
    "# Prefer WAV if it exists, otherwise use FLAC if it exists\n",
    "if wav_exists:\n",
    "    audio_path = wav_path\n",
    "    print(f\"Using existing WAV file: {wav_path}\")\n",
    "elif flac_exists:\n",
    "    audio_path = flac_path\n",
    "    print(f\"Using existing FLAC file: {flac_path}\")\n",
    "    \n",
    "    # Try to convert to WAV if FLAC exists but WAV doesn't\n",
    "    print(\"Converting FLAC to WAV format for better compatibility...\")\n",
    "    try:\n",
    "        # Load the audio file with librosa\n",
    "        audio_data, sample_rate = librosa.load(flac_path, sr=None)\n",
    "        \n",
    "        # Save as WAV using soundfile\n",
    "        sf.write(wav_path, audio_data, sample_rate)\n",
    "        print(f\"Converted audio saved to {wav_path}\")\n",
    "        audio_path = wav_path  # Use the newly created WAV file\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting audio: {e}\")\n",
    "        print(\"Using original FLAC file instead.\")\n",
    "else:\n",
    "    # Neither file exists, need to download\n",
    "    print(\"Sample audio not found. Downloading and extracting from archive...\")\n",
    "    \n",
    "    # Download the tarball\n",
    "    tarball_url = \"https://openslr.elda.org/resources/12/dev-clean.tar.gz\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".tar.gz\", delete=False) as temp_file:\n",
    "        print(f\"Downloading archive from {tarball_url}...\")\n",
    "        urlretrieve(tarball_url, temp_file.name)\n",
    "        tarball_path = temp_file.name\n",
    "    \n",
    "    # Extract the specific file we need\n",
    "    target_file_path = \"LibriSpeech/dev-clean/84/121123/84-121123-0001.flac\"\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(\"Extracting archive...\")\n",
    "        with tarfile.open(tarball_path, \"r:gz\") as tar:\n",
    "            # Extract only the file we need\n",
    "            member = tar.getmember(target_file_path)\n",
    "            tar.extract(member, path=temp_dir)\n",
    "        \n",
    "        # Move the extracted file to our sample directory\n",
    "        extracted_file = os.path.join(temp_dir, target_file_path)\n",
    "        shutil.copy(extracted_file, flac_path)\n",
    "    \n",
    "    # Clean up the tarball\n",
    "    os.unlink(tarball_path)\n",
    "    print(f\"Sample audio extracted to {flac_path}\")\n",
    "    \n",
    "    # Convert FLAC to WAV using librosa\n",
    "    print(\"Converting FLAC to WAV format...\")\n",
    "    try:\n",
    "        # Load the audio file with librosa\n",
    "        audio_data, sample_rate = librosa.load(flac_path, sr=None)\n",
    "        \n",
    "        # Save as WAV using soundfile\n",
    "        sf.write(wav_path, audio_data, sample_rate)\n",
    "        print(f\"Converted audio saved to {wav_path}\")\n",
    "        audio_path = wav_path  # Use the WAV file\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting audio: {e}\")\n",
    "        print(\"Using original FLAC file instead.\")\n",
    "        audio_path = flac_path\n",
    "\n",
    "print(f\"Using audio file: {audio_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Processing Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(file_path):\n",
    "    # Load audio using alternative method if torchaudio fails\n",
    "    try:\n",
    "        # Try torchaudio first\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "    except RuntimeError:\n",
    "        # Fall back to using librosa\n",
    "        print(f\"torchaudio failed to load {file_path}, trying librosa instead...\")\n",
    "        import librosa\n",
    "        import numpy as np\n",
    "        \n",
    "        # Load with librosa (automatically handles various formats including FLAC)\n",
    "        audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "        waveform = torch.from_numpy(audio_data).unsqueeze(0).float()\n",
    "        print(\"Successfully loaded audio with librosa\")\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "    \n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    return waveform.squeeze(), sample_rate\n",
    "\n",
    "# Load and process the audio\n",
    "waveform, sample_rate = process_audio(audio_path)\n",
    "\n",
    "# Display audio information\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Waveform shape: {waveform.shape}\")\n",
    "print(f\"Audio duration: {waveform.shape[0]/sample_rate:.2f} seconds\")\n",
    "\n",
    "# Play the audio\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained ASR Model\n",
    "\n",
    "We'll use the Wav2Vec 2.0 model from Facebook, which has been pre-trained on 960 hours of LibriSpeech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and processor\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Phoneme Probabilities\n",
    "\n",
    "Now we'll extract the logits from the model, which represent the probabilities of different phonemes at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phoneme_probs(waveform, sample_rate=16000):\n",
    "    # Process audio for model input\n",
    "    input_values = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "    input_values = input_values.to(device)\n",
    "    \n",
    "    # Get model outputs (without gradient calculation)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    return probs.cpu().squeeze(), processor.tokenizer.decoder\n",
    "\n",
    "# Get phoneme probabilities\n",
    "phoneme_probs, decoder = extract_phoneme_probs(waveform)\n",
    "print(f\"Shape of phoneme probabilities: {phoneme_probs.shape}\")\n",
    "print(f\"Number of time steps: {phoneme_probs.shape[0]}\")\n",
    "print(f\"Number of phoneme classes: {phoneme_probs.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Phoneme Activations\n",
    "\n",
    "Let's visualize the top phoneme activations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phoneme_activations(probs, decoder, top_k=5):\n",
    "    # Get top-k phonemes at each time step\n",
    "    top_probs, top_indices = torch.topk(probs, k=top_k, dim=1)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    top_probs = top_probs.numpy()\n",
    "    top_indices = top_indices.numpy()\n",
    "    \n",
    "    # Get phoneme labels\n",
    "    phoneme_map = {v: k for k, v in decoder.items()}\n",
    "    \n",
    "    # Create a time axis (assuming 50 frames per second for Wav2Vec 2.0)\n",
    "    time_steps = np.arange(top_probs.shape[0]) / 50\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot for a subset of time steps for clarity\n",
    "    start_idx = 0\n",
    "    end_idx = min(200, len(time_steps))  # Show first 4 seconds or less\n",
    "    \n",
    "    for i in range(top_k):\n",
    "        plt.plot(time_steps[start_idx:end_idx], \n",
    "                 top_probs[start_idx:end_idx, i], \n",
    "                 label=f\"Class {top_indices[0, i]} ({phoneme_map.get(top_indices[0, i], '')})\")\n",
    "    \n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Top Phoneme Activations Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize phoneme activations\n",
    "plot_phoneme_activations(phoneme_probs, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding to Phonemes and Text\n",
    "\n",
    "Now let's decode the model outputs to both phonemes and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_outputs(probs, decoder):\n",
    "    # Get the most likely phoneme at each time step\n",
    "    pred_ids = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    # Decode to phonemes (keeping all predictions)\n",
    "    phoneme_sequence = [decoder.get(id.item(), f\"[{id.item()}]\") for id in pred_ids]\n",
    "    \n",
    "    # Apply CTC decoding logic (collapse repeated tokens and remove blanks)\n",
    "    collapsed_phonemes = []\n",
    "    prev_id = -1\n",
    "    for id in pred_ids:\n",
    "        if id != prev_id and id != 0:  # 0 is usually the blank token in CTC\n",
    "            collapsed_phonemes.append(decoder.get(id.item(), f\"[{id.item()}]\"))\n",
    "        prev_id = id\n",
    "    \n",
    "    # Join phonemes to get the text\n",
    "    text = ''.join(collapsed_phonemes).replace('|', ' ')\n",
    "    \n",
    "    return phoneme_sequence, collapsed_phonemes, text\n",
    "\n",
    "# Decode outputs\n",
    "phoneme_sequence, collapsed_phonemes, text = decode_outputs(phoneme_probs, decoder)\n",
    "\n",
    "print(\"Full phoneme sequence (first 50 frames):\")\n",
    "print(phoneme_sequence[:50])\n",
    "print(\"\\nCollapsed phoneme sequence:\")\n",
    "print(collapsed_phonemes)\n",
    "print(\"\\nDecoded text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Model Fine-tuned for Phoneme Recognition\n",
    "\n",
    "For a more direct approach to phoneme recognition, we can use a model specifically fine-tuned for phoneme recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model fine-tuned for phoneme recognition\n",
    "# Note: This will download a different model\n",
    "phoneme_model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "print(f\"Loading phoneme model: {phoneme_model_name}\")\n",
    "\n",
    "try:\n",
    "    # Import specific processor class for this model\n",
    "    from transformers import Wav2Vec2ProcessorWithLM, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "    \n",
    "    # Load the model components separately\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(phoneme_model_name)\n",
    "    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(phoneme_model_name)\n",
    "    phoneme_processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "    phoneme_model = Wav2Vec2ForCTC.from_pretrained(phoneme_model_name).to(device)\n",
    "    print(\"Phoneme model loaded successfully!\")\n",
    "    \n",
    "    def transcribe_to_phonemes(waveform, sample_rate=16000):\n",
    "        # Process audio for model input\n",
    "        input_values = phoneme_processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "        input_values = input_values.to(device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = phoneme_model(input_values).logits\n",
    "        \n",
    "        # Decode phonemes\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        phoneme_string = phoneme_processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        return phoneme_string\n",
    "\n",
    "    # Get phoneme transcription\n",
    "    phoneme_transcription = transcribe_to_phonemes(waveform)\n",
    "    print(\"\\nPhoneme transcription:\")\n",
    "    print(phoneme_transcription)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading phoneme model: {e}\")\n",
    "    print(\"Skipping phoneme-specific model demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Phoneme Distributions\n",
    "\n",
    "Let's analyze the distribution of phonemes in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count phoneme occurrences\n",
    "from collections import Counter\n",
    "\n",
    "# Count non-blank phonemes\n",
    "phoneme_counts = Counter([p for p in collapsed_phonemes if p != ''])\n",
    "\n",
    "# Plot top 15 phonemes\n",
    "top_phonemes = phoneme_counts.most_common(15)\n",
    "phonemes, counts = zip(*top_phonemes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(phonemes, counts)\n",
    "plt.title('Top 15 Phonemes in Sample')\n",
    "plt.xlabel('Phoneme')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Phoneme Transcriptions\n",
    "\n",
    "Let's compare the different phoneme transcriptions side by side to see the differences between methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison of the different phoneme transcriptions\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Store the transcriptions in variables for comparison\n",
    "# Note: These will be populated when the cells above are run\n",
    "ctc_text = text  # From the CTC decoding section\n",
    "try:\n",
    "    specialized_text = phoneme_transcription  # From the specialized model section\n",
    "except NameError:\n",
    "    specialized_text = \"[Model failed to load]\"\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Wav2Vec2 Base with CTC Decoding', 'Specialized Phoneme Model'],\n",
    "    'Transcription': [ctc_text, specialized_text]\n",
    "})\n",
    "\n",
    "# Display the comparison\n",
    "display(HTML(comparison_df.to_html(index=False)))\n",
    "\n",
    "# Also show a more detailed comparison of the phoneme sequences\n",
    "print(\"\\nDetailed Phoneme Sequence Comparison:\")\n",
    "print(\"\\nWav2Vec2 Base with CTC Decoding:\")\n",
    "print(' '.join(collapsed_phonemes[:30]) + \"...\")\n",
    "\n",
    "try:\n",
    "    # For the specialized model, we might need to split the string into individual phonemes\n",
    "    if isinstance(specialized_text, str):\n",
    "        specialized_phonemes = list(specialized_text.replace(\" \", \"|SPACE|\"))\n",
    "        print(\"\\nSpecialized Phoneme Model:\")\n",
    "        print(' '.join(specialized_phonemes[:30]) + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not process specialized phonemes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Load and process audio files for ASR\n",
    "2. Extract phoneme probabilities from Wav2Vec 2.0 models\n",
    "3. Visualize phoneme activations over time\n",
    "4. Decode phoneme sequences to text\n",
    "5. Use models specifically fine-tuned for phoneme recognition\n",
    "6. Compare different phoneme transcription methods\n",
    "\n",
    "These techniques can be applied to various applications such as:\n",
    "- Studying pronunciation patterns\n",
    "- Developing language learning tools\n",
    "- Creating more interpretable ASR systems\n",
    "- Analyzing speech disorders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr-phonemes-venv",
   "language": "python",
   "name": "asr-phonemes-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
